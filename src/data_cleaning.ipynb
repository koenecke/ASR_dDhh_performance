{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c513cdd-448a-4cd2-91c8-f096dc103ba6",
   "metadata": {},
   "source": [
    "# d/Deaf & Hard of Hearing ASR study - Groundtruth Collection & Cleaning\n",
    "### Anna Choi, sc2359@cornell.edu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ef03be-8a11-4928-981e-aebc2a6c189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from jiwer import wer\n",
    "from whisper_normalizer.english import EnglishTextNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a39b05-bca4-462e-89a7-0ac59001d886",
   "metadata": {},
   "source": [
    "## 0. Convert audio files\n",
    "Exist extensions other than .wav in normal hearing files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdca87b-5873-48fa-aad3-89fbf485ccf8",
   "metadata": {},
   "source": [
    "linux command\n",
    "\n",
    "`for i in *.[WAV|MP3]; do ffmpeg -i \"$i\" \"${i%.*}.wav\"; done`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe27157-e86b-4fdb-a1b3-30bf398a3b08",
   "metadata": {},
   "source": [
    "## 1. Create `file_list.csv` and `groundtruth.csv`.\n",
    "\n",
    "`groundtruth.csv` file contains columns `subject_id`, `filename`, `passage_id`, `groundtruth`.\n",
    "\n",
    "Note: `S12NWCDR2.wav` wrong filename, `passage_id` should be changed to `NWS`. `S7IPEDR2.wav` is removed for not having the right reading passage. `NH4C16DR2.wav` and `NH4C17DR2.wav` should have each other's reading passage. `NH5C4DR2.wav` corrupted file. `NH2CGCDR1.wav` duplicate file. `N3DC3DR2.wav` duplicate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f697ad5-de71-4f11-9ee3-cddfe347098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 586 deaf participant files.\n",
      "There is a total of 318 normal hearing files.\n"
     ]
    }
   ],
   "source": [
    "def convert_bytes(num):\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return num, unit\n",
    "        num /= 1024.0\n",
    "\n",
    "def get_file_size(file_path):\n",
    "    try:\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_size, unit = convert_bytes(file_size)\n",
    "        return file_size if unit == 'MB' else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def generate_file_list(root_directory):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                match = re.search(r'(S\\d+|NH\\d+)([A-Z]+(?:[0-9]*))(DR[12])\\.wav', file)\n",
    "                if match:\n",
    "                    subject_id = match.group(1)\n",
    "                    passage_id = match.group(2)\n",
    "                    full_file_path = os.path.join(root, file)\n",
    "                    file_size = get_file_size(full_file_path)\n",
    "                    \n",
    "                    if file == 'S12NWCDR2.wav':\n",
    "                        passage_id = 'NWS'\n",
    "\n",
    "                    file_list.append({\n",
    "                        'subject_id': subject_id,\n",
    "                        'filename': file,\n",
    "                        'passage_id': passage_id,\n",
    "                        'size': file_size\n",
    "                    })\n",
    "    return file_list\n",
    "\n",
    "deaf_directory = '../../../deaf/'\n",
    "hearing_directory = '../../../hearing/'\n",
    "\n",
    "deaf_file_list = generate_file_list(deaf_directory)\n",
    "hearing_file_list = generate_file_list(hearing_directory)\n",
    "\n",
    "deaf_file_df = pd.DataFrame(deaf_file_list)\n",
    "hearing_file_df = pd.DataFrame(hearing_file_list)\n",
    "\n",
    "deaf_file_df = deaf_file_df[deaf_file_df['filename'] != 'S7IPEDR2.wav']\n",
    "deaf_file_df.loc[deaf_file_df['filename'] == 'S12NWCDR2.wav', 'passage_id'] = 'NWS'\n",
    "deaf_file_df = deaf_file_df[deaf_file_df['subject_id'] != 'S11']\n",
    "hearing_file_df = hearing_file_df[hearing_file_df['filename'] != 'NH5C4DR2.wav']\n",
    "hearing_file_df = hearing_file_df[hearing_file_df['filename'] != 'NH2CGCDR1.wav']\n",
    "hearing_file_df = hearing_file_df[hearing_file_df['filename'] != 'NH3DC3DR2.wav']\n",
    "hearing_file_df.loc[hearing_file_df['filename'] == 'NH4C16DR2.wav', 'passage_id'] = 'C17'\n",
    "hearing_file_df.loc[hearing_file_df['filename'] == 'NH4C17DR2.wav', 'passage_id'] = 'C16'\n",
    "\n",
    "deaf_file_df.to_csv('../../data/deaf_file_list.csv', encoding = 'utf-8-sig', index=False)\n",
    "hearing_file_df.to_csv('../../data/hearing_file_list.csv', encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "deaf_file_count = len(deaf_file_df)\n",
    "hearing_file_count = len(hearing_file_df)\n",
    "\n",
    "print(f\"There is a total of {deaf_file_count} deaf participant files.\")\n",
    "print(f\"There is a total of {hearing_file_count} normal hearing files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6eb4c86-abca-4223-96aa-15411d57627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_save(file_list_path, passage_path, output_path):\n",
    "    file_list_df = pd.read_csv(file_list_path)\n",
    "    passage_df = pd.read_csv(passage_path)\n",
    "    merged_df = pd.merge(file_list_df, passage_df[['passage_id', 'groundtruth']], on='passage_id', how='left')\n",
    "    \n",
    "    null_values = merged_df[merged_df['groundtruth'].isnull()]\n",
    "    if not null_values.empty:\n",
    "        print(\"Warning: Null values found in the 'groundtruth' column. Handling them...\")\n",
    "        merged_df['groundtruth'].fillna('', inplace=True)\n",
    "    \n",
    "    merged_df.to_csv(output_path, encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "deaf_file_list_path = '../../data/deaf_file_list.csv'\n",
    "deaf_passage_path = '../../data/passage.csv' \n",
    "deaf_output_path = '../../data/deaf_groundtruth.csv'\n",
    "merge_and_save(deaf_file_list_path, deaf_passage_path, deaf_output_path)\n",
    "\n",
    "hearing_file_list_path = '../../data/hearing_file_list.csv'\n",
    "hearing_passage_path = '../../data/passage.csv' \n",
    "hearing_output_path = '../../data/hearing_groundtruth.csv'\n",
    "merge_and_save(hearing_file_list_path, hearing_passage_path, hearing_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452dd8e-7b08-499c-b510-b8ca9d663f09",
   "metadata": {},
   "source": [
    "## 2. Create `all.csv` file\n",
    "\n",
    "`groundtruth_edited.csv` contains manual edits to the groundtruth especially regarding speech errors or individual variations.\n",
    "\n",
    "`all.csv` files contain transcripts from Amazon Web Service (`AWS`), Microsoft Azure (`Azure`), OpenAI Whisper (`Whisper`), and Google Chirp (`GoogleChirp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b93b011-6ae9-41d7-a2c3-80c780ffa593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_df(prefix, merged_df):\n",
    "    date_placeholder = '*'\n",
    "    \n",
    "    aws_json_path = glob.glob(f'../../data/ASR transcripts/{date_placeholder}_{prefix}_AWS_transcript.json')[0]\n",
    "    azure_json_path = glob.glob(f'../../data/ASR transcripts/{date_placeholder}_{prefix}_Azure_transcript.json')[0]\n",
    "    whisper_json_path = glob.glob(f'../../data/ASR transcripts/{date_placeholder}_{prefix}_Whisper_transcript.json')[0]\n",
    "    google_chirp_json_path = glob.glob(f'../../data/ASR transcripts/{date_placeholder}_{prefix}_GoogleChirp_transcript.json')[0]\n",
    "\n",
    "    with open(aws_json_path, 'r') as f:\n",
    "        aws_dict = json.load(f)\n",
    "    with open(azure_json_path, 'r') as f:\n",
    "        azure_dict = json.load(f)\n",
    "    with open(whisper_json_path, 'r') as f:\n",
    "        whisper_dict = json.load(f)\n",
    "    with open(google_chirp_json_path, 'r') as f:\n",
    "        google_chirp_dict = json.load(f)\n",
    "    \n",
    "    aws_df = pd.DataFrame(aws_dict)\n",
    "    azure_df = pd.DataFrame(azure_dict)\n",
    "    whisper_df = pd.DataFrame(whisper_dict)\n",
    "    google_chirp_df = pd.DataFrame(google_chirp_dict)\n",
    "\n",
    "    aws_df.rename(columns={'aws_transcription': 'AWS'}, inplace=True)\n",
    "    azure_df.rename(columns={'transcript': 'Azure'}, inplace=True)\n",
    "    whisper_df.rename(columns={'transcript': 'Whisper'}, inplace=True)\n",
    "    google_chirp_df.rename(columns={'transcript': 'GoogleChirp'}, inplace=True)\n",
    "\n",
    "    final_df = merged_df.copy()\n",
    "    final_df = pd.merge(final_df, aws_df, left_on='filename', right_on='segment_name', how='left').drop('segment_name', axis=1)\n",
    "    final_df = pd.merge(final_df, azure_df, on='filename', how='left')\n",
    "    final_df = pd.merge(final_df, whisper_df, on='filename', how='left')\n",
    "    final_df = pd.merge(final_df, google_chirp_df, on='filename', how='left')\n",
    "\n",
    "    final_df.to_csv(f'../../data/{prefix}_all.csv', encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "merged_df_deaf = pd.read_csv('../../data/deaf_groundtruth_edited.csv')\n",
    "generate_final_df('deaf', merged_df_deaf)\n",
    "\n",
    "merged_df_hearing = pd.read_csv('../../data/hearing_groundtruth_edited.csv')\n",
    "generate_final_df('hearing', merged_df_hearing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e12a26f-8fef-4b54-81d5-db9f91063b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique subjects in deaf dataset: 25\n",
      "Unique subjects in hearing dataset: 9\n"
     ]
    }
   ],
   "source": [
    "deaf_df = pd.read_csv(\"../../data/deaf_all.csv\", encoding = 'utf-8-sig')\n",
    "hearing_df = pd.read_csv(\"../../data/hearing_all.csv\", encoding = 'utf-8-sig')\n",
    "\n",
    "unique_subjects_deaf = deaf_df['subject_id'].nunique()\n",
    "\n",
    "unique_subjects_hearing = hearing_df['subject_id'].nunique()\n",
    "\n",
    "print(f\"Unique subjects in deaf dataset: {unique_subjects_deaf}\")\n",
    "print(f\"Unique subjects in hearing dataset: {unique_subjects_hearing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a470a2-6993-406d-95b5-3cfd9a28350d",
   "metadata": {},
   "source": [
    "## 3. Create `all_calc.csv` file\n",
    "`all_calc.csv` contains the columns `{API}_WER` where the WER for each API is calculated.\n",
    "The same cleaning is done for all of groundtruth and the four APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec97fbd-a271-48a5-a757-ec5396544041",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_normalizer = EnglishTextNormalizer()\n",
    "\n",
    "filler_words = ['um', 'umm','uh', 'mhm', 'mm', 'ugh','uhhuh','mm-hmm',\"uhh\",\"mmhmm\",\"uh-huh\",\"uh-hmm\",\"uh-hm\",\"uh-hm\",\"hm\",\"hmm\",\"emmm\"]\n",
    "\n",
    "def general_clean(text):\n",
    "    resultwords  = [word.lower() for word in text.split() if re.sub('\\,','',word.lower()) not in filler_words] \n",
    "    result = ' '.join(resultwords) \n",
    "    result = re.sub(r'[^\\w\\s]|_', ' ',result) \n",
    "    result = re.sub(\"\\s+\",\" \",''.join(result))\n",
    "    return result\n",
    "\n",
    "def apply_contraction(text, replacements):\n",
    "    split_words = text.split()\n",
    "    for i in range(len(split_words)):\n",
    "        if split_words[i].lower() in replacements.keys():\n",
    "            split_words[i] = replacements[split_words[i].lower()]\n",
    "    text = ' '.join(split_words)   \n",
    "    return text\n",
    "\n",
    "def apply_replacements(text, replacements):\n",
    "    for pattern, replacement in replacements.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text\n",
    "\n",
    "def apply_simple_replacements(text, replacements):\n",
    "    for original, replacement in replacements.items():\n",
    "        text = text.replace(original, replacement)\n",
    "    return text\n",
    "\n",
    "def remove_spaces_between_numbers(text):\n",
    "    return re.sub(r'(\\d{1,2})\\s+(\\d{1,2})', r'\\1\\2', text)\n",
    "\n",
    "def clean_text(text, passage_id):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    contraction_replacements = {\n",
    "        \"gonna\":\"going to\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"cuz\":\"cause\",\n",
    "        \"ok\":\"okay\",\n",
    "        \"asu\":\"a s u\",\n",
    "        \"otpt\":\"o t p t\",\n",
    "        \"tv\":\"t v\",\n",
    "        \"phd\":\"p h d\",\n",
    "        \"ph.d\":\"p h d\",\n",
    "        \"ot\":\"o t\",\n",
    "        \"pt\":\"p t\",\n",
    "        \"er\":\"e r\",\n",
    "        \"t-e-s-t\":\"t e s t\",\n",
    "        \"function\":\"func tion\",\n",
    "        \"growin\":\"growing\",\n",
    "        \"ft\":\"feet\",\n",
    "        \"dc\":\"d c\",\n",
    "        \"unc\":\"u n c\",\n",
    "        \"kilometers\":\"km\",\n",
    "        \"hafta\":\"have to\",\n",
    "        \"useta\":\"used to\",\n",
    "        \"bout\":\"about\",\n",
    "        \"windowsill\":\"window sill\",\n",
    "        \"heartbeat\":\"heart beat\"\n",
    "    }\n",
    "\n",
    "    general_replacements = {\n",
    "        r'\\bhol\\b': 'hall',\n",
    "        r'\\b(holis|hollice)\\b': 'hollis',\n",
    "        r'\\banne\\b': 'ann',\n",
    "        r'\\b(wit|witt|whitt)\\b': 'whit',\n",
    "        r'\\bstatt\\b': 'stat',\n",
    "        r'\\bstatts\\b': 'stats',\n",
    "        r'\\bblares\\b': 'blairs',\n",
    "        r'\\b(davy|davey|davie)\\b': 'david',\n",
    "        r'\\bcrocket\\b': 'crockett',\n",
    "        r'\\bax\\b': 'axe',\n",
    "        r'\\b(kama|koma)\\b': 'comma',\n",
    "        r'\\bchatachuchi\\b': 'chattahoochee',\n",
    "        r'\\b(techumish|tekimish|tukamush|tekimish|tookamush)\\b': 'tecumseh',\n",
    "        r'\\b(under brush|under rush|underrushed)\\b': 'underbrush'\n",
    "    }\n",
    "    passage_specific_replacements = {\n",
    "        'C1': {\n",
    "            r'\\b(sh sh sh|sh h|sh sh|sh)\\b': '',\n",
    "            r'\\b(libs|lips)\\b': 'lids',\n",
    "        },\n",
    "        'CGC': {r'\\b(ms|miss)\\b': 'missus'},\n",
    "        'DC8': {r'\\b(grim stools|grimstalls)\\b': 'grimstills'},\n",
    "        'C5': {r'\\b(e r oop|ar up|oop|whoop|a whoop|a rope|air up|arup|roop|roof|hoop|erupt|erp)\\b': 'oof'}\n",
    "    }\n",
    "    \n",
    "    simple_replacements = {\n",
    "        ' ve ': ' have ',\n",
    "        \"getup\":\"get up\",\n",
    "        'shame faced': 'shamefaced',\n",
    "        'plot': 'plop',\n",
    "        'sun glow': 'sunglow',\n",
    "        'billygoat': 'billy goat',\n",
    "        'bob white': 'bobwhite',\n",
    "        'hail storm': 'hailstorm',\n",
    "        'sun bonnet': 'sunbonnet',\n",
    "        'crisscross': 'criss cross',\n",
    "        'mole hill': 'molehill',\n",
    "        'wheat field': 'wheatfield',\n",
    "        'kitt e e': 'kitty',\n",
    "        'fella': 'fellow',\n",
    "        'look out': 'lookout',\n",
    "        'anymore': 'any more',\n",
    "        'nightwatch': 'night watch',\n",
    "        'clothesline': 'clothes line',\n",
    "        'gol lee': 'golly',\n",
    "        'dog fights': 'dogfights',\n",
    "        'cock fights': 'cockfights',\n",
    "        'any time': 'anytime',\n",
    "        'grist mills': 'gristmills',\n",
    "        'grist mill': 'gristmill',\n",
    "        'superimposition': 'super imposition',\n",
    "        'cold blooded': 'coldblooded',\n",
    "    }\n",
    "    \n",
    "    text = text.replace(\"'\",\" \")\n",
    "    text = text.replace('one third', '1/3').replace('one-third', '1/3').replace(\"one. Third\", \"1/3\")\n",
    "    text = english_normalizer(text)\n",
    "    text = general_clean(text)\n",
    "    text = apply_contraction(text, contraction_replacements)\n",
    "    text = apply_simple_replacements(text, simple_replacements)\n",
    "    text = apply_replacements(text, general_replacements)\n",
    "    if passage_id in passage_specific_replacements:\n",
    "        text = apply_replacements(text, passage_specific_replacements[passage_id])\n",
    "    text = remove_spaces_between_numbers(text)\n",
    "    if passage_id == 'DC9':\n",
    "        text = text.replace('100 and 7', '107').replace('18111812', '1811 1812')\n",
    "    if passage_id == 'C14':\n",
    "        text = re.sub(r'\\b150\\b', '15 0', text)\n",
    "    text = text.replace('318', '3 18')\n",
    "    text = re.sub(r'\\b3 18\\b', '3 18th', text)\n",
    "    text = text.replace('one 24', '124').replace('one 39', '139')\n",
    "    text = text.replace('1232', '12 32')\n",
    "    text = text.replace('124139', '124 139')\n",
    "    text = text.replace('111811', '11 1811')\n",
    "    text = text.replace('181812', '18 1812')\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def clean_final_df(final_df, asr_columns=['groundtruth', 'AWS', 'Azure', 'Whisper', 'GoogleChirp']):\n",
    "    for col in asr_columns:\n",
    "        clean_col_name = f\"{col}_clean\"\n",
    "        final_df[clean_col_name] = final_df.apply(lambda row: clean_text(row[col], row['passage_id']), axis=1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46e16a0-527a-4dbe-af6b-03e7f6f641b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer_jiwer(groundtruth, hypothesis):\n",
    "    if pd.isna(hypothesis):\n",
    "        return 1.0\n",
    "    if hypothesis.strip() == '':\n",
    "        return 1.0\n",
    "    return wer(groundtruth, hypothesis)\n",
    "\n",
    "def calculate_wer_for_df(final_df, asr_columns=['AWS_clean', 'Azure_clean', 'Whisper_clean', 'GoogleChirp_clean']):\n",
    "    for col in asr_columns:\n",
    "        asr_provider = col.split('_')[0]\n",
    "        wer_col_name = f\"{asr_provider}_WER\"\n",
    "        final_df[wer_col_name] = final_df.apply(lambda row: calculate_wer_jiwer(row['groundtruth_clean'], row[col]), axis=1)\n",
    "    return final_df\n",
    "\n",
    "final_df_deaf = pd.read_csv('../../data/deaf_all.csv', encoding = 'utf-8-sig')\n",
    "clean_df_deaf = clean_final_df(final_df_deaf)\n",
    "wer_df_deaf = calculate_wer_for_df(clean_df_deaf)\n",
    "wer_df_deaf.to_csv('../../data/deaf_all_calc.csv', encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "final_df_hearing = pd.read_csv('../../data/hearing_all.csv', encoding = 'utf-8-sig')\n",
    "clean_df_hearing = clean_final_df(final_df_hearing)\n",
    "wer_df_hearing = calculate_wer_for_df(clean_df_hearing)\n",
    "wer_df_hearing.to_csv('../../data/hearing_all_calc.csv', encoding = 'utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7720b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df = pd.read_csv('../../data/demographics.csv')\n",
    "\n",
    "def clean_age_onset(age_of_onset):\n",
    "    if age_of_onset in [\"27 month\", \"2years\"]:\n",
    "        return 2\n",
    "    elif age_of_onset in [\"at birth\", \"4 month\"]:\n",
    "        return 0\n",
    "    elif age_of_onset == \"5 years\":\n",
    "        return 5\n",
    "    elif age_of_onset == \"7 years\":\n",
    "        return 7\n",
    "    else:\n",
    "        return age_of_onset\n",
    "\n",
    "if 'Group' not in demographics_df.columns:\n",
    "    demographics_df['Group'] = None\n",
    "    \n",
    "demographics_df['age_onset_clean'] = demographics_df['age_of_onset'].apply(clean_age_onset)\n",
    "merged_df_deaf = pd.merge(demographics_df, wer_df_deaf, on='subject_id', how='inner')\n",
    "merged_df_hearing = pd.merge(demographics_df, wer_df_hearing, on='subject_id', how='inner')\n",
    "merged_all = pd.concat([merged_df_deaf, merged_df_hearing], ignore_index=True)\n",
    "merged_all.fillna('NA', inplace=True)\n",
    "merged_all['num_words'] = merged_all['groundtruth_clean'].apply(lambda x: len(str(x).split()))\n",
    "column_order = [\n",
    "    'subject_id', 'filename', 'passage_id', 'size', 'groundtruth', 'AWS', 'Azure', 'Whisper', 'GoogleChirp',\n",
    "    'groundtruth_clean', 'AWS_clean', 'Azure_clean', 'Whisper_clean', 'GoogleChirp_clean',\n",
    "    'AWS_WER', 'Azure_WER', 'Whisper_WER', 'GoogleChirp_WER', 'Group', 'age', 'gender', 'age_of_onset',\n",
    "    'right_amplification_start', 'left_amplification_start', 'right_amplification_type', 'left_amplification_type',\n",
    "    'right_amplification_model', 'left_amplification_model', 'speech_intelligibility', 'onset_hearing_loss',\n",
    "    'communication_mode', 'age_onset_clean', 'num_words'\n",
    "]\n",
    "\n",
    "merged_all = merged_all[column_order]\n",
    "\n",
    "merged_all.to_csv('../../data/SPAL_data.csv', encoding = 'utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
